{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613d549f-b070-4d1c-8d8c-d6dddde4e1c8",
   "metadata": {},
   "source": [
    "# NLP Assignment 1\n",
    "Author: Vincent Itucal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973c0b20-0491-4650-806d-ddb93b3114da",
   "metadata": {},
   "source": [
    "## Assignment -->  Use Case 1: Removing subsequent occurrence of words.\n",
    "\n",
    "Removing subsequent occurrences of words (also known as deduplication of adjacent duplicate words) is a common preprocessing step in NLP. This task is important because: \n",
    "1. Repeated words can distort text analysis, especially in tasks like text summarization, sentiment analysis, and language modeling.\n",
    "2. Removing redundant words improves the readability of the text, making it more coherent.\n",
    "3. Reducing noise in the text data can improve the performance of machine learning models.\n",
    "\n",
    "Input:\n",
    "A single string text that may contain multiple sentences and words. Words are separated by spaces.\n",
    "\n",
    "Output:\n",
    "A single string with subsequent duplicate words removed.\n",
    "\n",
    "Constraints:\n",
    "The input string can be empty.\n",
    "The words are case-sensitive, meaning \"Word\" and \"word\" are considered different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8175b408-4dfc-4d09-8c7d-b0f237726563",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T19:31:24.037419Z",
     "iopub.status.busy": "2025-02-03T19:31:24.036419Z",
     "iopub.status.idle": "2025-02-03T19:31:28.960091Z",
     "shell.execute_reply": "2025-02-03T19:31:28.959577Z",
     "shell.execute_reply.started": "2025-02-03T19:31:24.037419Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\vsitu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vsitu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk\n",
    "import spacy\n",
    "# nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2781f563-9a5a-41dd-95f8-052e9bdad28a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T19:31:28.961099Z",
     "iopub.status.busy": "2025-02-03T19:31:28.960091Z",
     "iopub.status.idle": "2025-02-03T19:31:28.966903Z",
     "shell.execute_reply": "2025-02-03T19:31:28.966397Z",
     "shell.execute_reply.started": "2025-02-03T19:31:28.961099Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_adjacent_duplicates_nltk(text):\n",
    "    if not text:\n",
    "        return \"\"  # Return empty string if input is empty\n",
    "\n",
    "    sentences = sent_tokenize(text)  # Split text into sentences\n",
    "    detokenizer = TreebankWordDetokenizer()\n",
    "    cleaned_sentences = []\n",
    "    prev_sentence = None\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)  # Tokenize sentence into words\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        # Remove adjacent duplicate words (Case-Sensitive)\n",
    "        filtered_words = [words[0]]\n",
    "        for i in range(1, len(words)):\n",
    "            if words[i] != words[i - 1]:  # Case-sensitive comparison\n",
    "                filtered_words.append(words[i])\n",
    "\n",
    "        cleaned_sentence = detokenizer.detokenize(filtered_words)  # Proper spacing\n",
    "\n",
    "        # Avoid adding duplicate adjacent sentences (Case-Sensitive)\n",
    "        if cleaned_sentence != prev_sentence:\n",
    "            cleaned_sentences.append(cleaned_sentence)\n",
    "            prev_sentence = cleaned_sentence  # Keep case-sensitive tracking\n",
    "\n",
    "    return \" \".join(cleaned_sentences)  # Proper spacing between sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d611794-0286-449b-9a55-36b7dfc1b101",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T19:31:28.967909Z",
     "iopub.status.busy": "2025-02-03T19:31:28.967909Z",
     "iopub.status.idle": "2025-02-03T19:31:28.992312Z",
     "shell.execute_reply": "2025-02-03T19:31:28.992312Z",
     "shell.execute_reply.started": "2025-02-03T19:31:28.967909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world, World. Hello world, world. This is a test!\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello world, World. Hello world, world. This is is a test test!  This is is a test test!\"\n",
    "print(remove_adjacent_duplicates_nltk(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c975a8-5d7c-482f-9006-9fe96467665d",
   "metadata": {},
   "source": [
    "## Assignment --> Use Case 2: Adding Custom Stop Words to `nltk` and `spacy`\n",
    "\n",
    "\n",
    "Adding custom stop words is a crucial preprocessing step in NLP. This task is important because:\n",
    "\n",
    "Customizing stop words allows for more flexible and relevant text cleaning tailored to specific use cases.\n",
    "Adding domain-specific stop words improves the performance of text analysis and machine learning models by removing irrelevant terms.\n",
    "Enhances the readability and coherence of the text by eliminating non-essential words.\n",
    "\n",
    "Objective:\n",
    "Extend the default stop words list in both `nltk` and `spacy` by adding custom stop words.\n",
    "\n",
    "Input:\n",
    "A list of custom stop words to be added to the existing stop words list in `nltk` and `spacy.\n",
    "\n",
    "Output:\n",
    "A function that takes a string and returns the text with both default and custom stop words removed.\n",
    "\n",
    "Constraints:\n",
    "The input string can be empty.\n",
    "The words are case-sensitive, meaning \"Word\" and \"word\" are considered different.\n",
    "\n",
    "Instructions:\n",
    "Add custom stop words to `nltk`'s default stop words list.\n",
    "Add custom stop words to `spacy`'s default stop words list.\n",
    "Remove stop words from a given text using the updated stop words list for both `nltk` and `spacy.\n",
    "\n",
    "**Note: Please ensure that the custom stop words you add are unique to your implementation. When testing and checking your notebooks, I will include these specific words to ensure they have been correctly added to your stop words list.**\n",
    "\n",
    "Custom Stop Words to Use:    \n",
    "\"customword1\";  \n",
    "\"customword2\";  \n",
    "\"customword3\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2be6a7-432d-40f0-8efa-eea46ca02904",
   "metadata": {},
   "source": [
    "### Add custom words here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf76b6c7-48b5-48b3-aa0a-8f6c69eebf16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T19:31:28.993325Z",
     "iopub.status.busy": "2025-02-03T19:31:28.993325Z",
     "iopub.status.idle": "2025-02-03T19:31:28.997319Z",
     "shell.execute_reply": "2025-02-03T19:31:28.997319Z",
     "shell.execute_reply.started": "2025-02-03T19:31:28.993325Z"
    }
   },
   "outputs": [],
   "source": [
    "custom_stop_words = {\"cat\", \"dog\", \"sheep\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "443bab28-2b17-4361-9059-298f0e468ed3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T19:31:28.998339Z",
     "iopub.status.busy": "2025-02-03T19:31:28.998339Z",
     "iopub.status.idle": "2025-02-03T19:31:29.707471Z",
     "shell.execute_reply": "2025-02-03T19:31:29.707471Z",
     "shell.execute_reply.started": "2025-02-03T19:31:28.998339Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Extend NLTK stop words\n",
    "nltk_stop_words = set(stopwords.words(\"english\")).union(custom_stop_words)\n",
    "\n",
    "# Extend spaCy stop words\n",
    "for word in custom_stop_words:\n",
    "    nlp.Defaults.stop_words.add(word)\n",
    "    nlp.vocab[word].is_stop = True\n",
    "\n",
    "def remove_stop_words_nltk(text: str) -> str:\n",
    "    \"\"\"Removes stop words from the given text using nltk.\"\"\"\n",
    "    if not text.strip():  # Handle empty input\n",
    "        return \"\"\n",
    "\n",
    "    # Tokenize using NLTK\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words using NLTK's list\n",
    "    filtered_words = [word for word in words if word.lower() not in nltk_stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "def remove_stop_words_spacy(text: str) -> str:\n",
    "    \"\"\"Removes stop words from the given text using spaCy.\"\"\"\n",
    "    if not text.strip():  # Handle empty input\n",
    "        return \"\"\n",
    "\n",
    "    # Process text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41126851-c2e4-4a22-8ef2-ad0323bdfc87",
   "metadata": {},
   "source": [
    "### nltk version test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a58b10b-2665-44b5-b650-f32f0b46e904",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T19:31:29.708476Z",
     "iopub.status.busy": "2025-02-03T19:31:29.708476Z",
     "iopub.status.idle": "2025-02-03T19:31:29.713141Z",
     "shell.execute_reply": "2025-02-03T19:31:29.712628Z",
     "shell.execute_reply.started": "2025-02-03T19:31:29.708476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'text', ',', ',', 'words', '.']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"This is a sample text with cat, dog, sheep and other words.\"\n",
    "cleaned_text = remove_stop_words_nltk(text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3921b2f-214a-4a7b-890e-7473b85a70ab",
   "metadata": {},
   "source": [
    "### spacy version test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8f7345e-cc0e-4e21-8b32-0c4c32e293f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T19:31:29.714147Z",
     "iopub.status.busy": "2025-02-03T19:31:29.714147Z",
     "iopub.status.idle": "2025-02-03T19:31:29.736509Z",
     "shell.execute_reply": "2025-02-03T19:31:29.736509Z",
     "shell.execute_reply.started": "2025-02-03T19:31:29.714147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'text', ',', ',', 'words', '.']\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = remove_stop_words_spacy(text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b006013-8b9b-47b1-9e84-0aa4a4a0871f",
   "metadata": {},
   "source": [
    "## Assignment --> Use Case 3: `nltk` Stemming\n",
    "\n",
    "Objective:\n",
    "\n",
    "Understand and compare the stemming techniques. Determine when each stemming technique is appropriate to use based on the context and requirements.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Apply stemming using `PorterStemmer`, `LancasterStemmer`, and `SnowballStemmer`.\n",
    "\n",
    "Compare the results and analyze the differences.\n",
    "\n",
    "Write code to demonstrate the stemming process for each stemmer.\n",
    "Provide example text and show the output of each stemming process.\n",
    "\n",
    "Analysis:\n",
    "\n",
    "Discuss the differences between the stemmers.\n",
    "Explain when one stemmer might be more appropriate than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6621995-f6c4-411b-84bc-afbf8902a145",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T19:31:29.737514Z",
     "iopub.status.busy": "2025-02-03T19:31:29.737514Z",
     "iopub.status.idle": "2025-02-03T19:31:29.745220Z",
     "shell.execute_reply": "2025-02-03T19:31:29.745220Z",
     "shell.execute_reply.started": "2025-02-03T19:31:29.737514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: ['running', 'jumps', 'happily', 'flies', 'better', 'swimming', 'flying']\n",
      "Porter Stemmer: ['run', 'jump', 'happili', 'fli', 'better', 'swim', 'fli']\n",
      "Lancaster Stemmer: ['run', 'jump', 'happy', 'fli', 'bet', 'swim', 'fly']\n",
      "Snowball Stemmer: ['run', 'jump', 'happili', 'fli', 'better', 'swim', 'fli']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "# Example text\n",
    "text = [\"running\", \"jumps\", \"happily\", \"flies\", \"better\", \"swimming\", \"flying\"]\n",
    "\n",
    "# Initialize the stemmers\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Apply stemming using each stemmer\n",
    "porter_stemmed = [porter_stemmer.stem(word) for word in text]\n",
    "lancaster_stemmed = [lancaster_stemmer.stem(word) for word in text]\n",
    "snowball_stemmed = [snowball_stemmer.stem(word) for word in text]\n",
    "\n",
    "# Print the results\n",
    "print(\"Original text:\", text)\n",
    "print(\"Porter Stemmer:\", porter_stemmed)\n",
    "print(\"Lancaster Stemmer:\", lancaster_stemmed)\n",
    "print(\"Snowball Stemmer:\", snowball_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ee6b46-27df-45d0-90ba-78253e152478",
   "metadata": {},
   "source": [
    "- <b>Porter Stemmer</b>\n",
    "    - Essentially, this stemmer classifies every character in a given token as either a consonant (c) or vowel (v), grouping subsequent consonants as C and subsequent vowels as V. The stemmer thus represents every word token as a combination of consonant and vowel groups. Once enumerated this way, the stemmer runs each word token through a list of rules that specify ending characters to remove according to the number of vowel-consonant groups in a token.<sup>[1]</sup>\n",
    "    - Foundation for subsequent algorithms\n",
    "    - Only supports the English language\n",
    "    - Use when you need a simple, widley used and relatively accurate stemmer.\n",
    "- <b>Snowball Stemmer</b>\n",
    "    - Snowball stemmer is an updated version of the Porter stemmer. While it aims to enforce a more robust set of rules for determining suffix removal, it nevertheless remains prone to many of the same errors\n",
    "    - The snowball stemmer presenting the English language stemmer is called Porter2\n",
    "    - Supports multiple languages, including English, Russian, Danish, French, Finnish, German, Italian, Hungarian, Portuguese, Norwegian, Swedish, and Spanish.\n",
    "    - Use when working with multiple languages or needing a balance between Porter’s conservatism and Lancaster’s aggressiveness.\n",
    "- <b>Lancaster Stemmer</b>\n",
    "    - Also known as the Paice stemmer—is the most aggressive of English language stemmers.\n",
    "    - The Lancaster stemmer contains a list of over 100 rules that dictate which ending character strings, if present, to replace with other strings, if any. The stemmer iterates through each word token, checking it against all the rules. If the token’s ending string matches that of a rule, the algorithm enacts the rule’s described operation and then runs the new, transformed word through all of the rules again. The stemmer iterates through all of the rules until a given token passes them all without being transformed<sup>[2]</sup>\n",
    "    - Only supports the English language\n",
    "    - Use when you need aggressive stemming but can tolerate some over-stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b496b-9a69-47ea-ad1d-e88718511d24",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49201fa-e609-410d-afda-73fb741af39e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T19:13:32.987674Z",
     "iopub.status.busy": "2025-02-03T19:13:32.985675Z",
     "iopub.status.idle": "2025-02-03T19:13:33.016295Z",
     "shell.execute_reply": "2025-02-03T19:13:33.015284Z",
     "shell.execute_reply.started": "2025-02-03T19:13:32.986675Z"
    }
   },
   "source": [
    "[1] Martin Porter, \"An algorithm for suffix stripping\", Program: electronic library and information systems, Vol. 14, No. 3, 1980, pp. 130-137, https://www.emerald.com/insight/content/doi/10.1108/eb046814/full/html \n",
    "\n",
    "[2] 12 Chris Paice, “Another stemmer,\" ACM SIGIR Forum, Vol. 24, No. 3, 1990, pp. 56–61, https://dl.acm.org/doi/10.1145/101306.101310\n",
    "\n",
    "[3] https://www.ibm.com/think/topics/stemming#:~:text=The%20Snowball%20stemmer%20differs%20from,%2C%20French%2C%20and%20even%20Russian.\n",
    "\n",
    "[4] https://towardsai.net/p/l/stemming-porter-vs-snowball-vs-lancaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4ea25f-af45-4e1c-b018-9f2616fac736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
