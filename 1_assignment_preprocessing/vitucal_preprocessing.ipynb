{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613d549f-b070-4d1c-8d8c-d6dddde4e1c8",
   "metadata": {},
   "source": [
    "# NLP Assignment 1\n",
    "Author: Vincent Itucal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973c0b20-0491-4650-806d-ddb93b3114da",
   "metadata": {},
   "source": [
    "## Assignment -->  Use Case 1: Removing subsequent occurrence of words.\n",
    "\n",
    "Removing subsequent occurrences of words (also known as deduplication of adjacent duplicate words) is a common preprocessing step in NLP. This task is important because: \n",
    "1. Repeated words can distort text analysis, especially in tasks like text summarization, sentiment analysis, and language modeling.\n",
    "2. Removing redundant words improves the readability of the text, making it more coherent.\n",
    "3. Reducing noise in the text data can improve the performance of machine learning models.\n",
    "\n",
    "Input:\n",
    "A single string text that may contain multiple sentences and words. Words are separated by spaces.\n",
    "\n",
    "Output:\n",
    "A single string with subsequent duplicate words removed.\n",
    "\n",
    "Constraints:\n",
    "The input string can be empty.\n",
    "The words are case-sensitive, meaning \"Word\" and \"word\" are considered different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8175b408-4dfc-4d09-8c7d-b0f237726563",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T07:35:32.267064Z",
     "iopub.status.busy": "2025-02-03T07:35:32.267064Z",
     "iopub.status.idle": "2025-02-03T07:35:35.693208Z",
     "shell.execute_reply": "2025-02-03T07:35:35.693208Z",
     "shell.execute_reply.started": "2025-02-03T07:35:32.267064Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vsitu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\vsitu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vsitu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk\n",
    "import spacy\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2781f563-9a5a-41dd-95f8-052e9bdad28a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T06:50:46.618960Z",
     "iopub.status.busy": "2025-02-03T06:50:46.618960Z",
     "iopub.status.idle": "2025-02-03T06:50:46.625471Z",
     "shell.execute_reply": "2025-02-03T06:50:46.624965Z",
     "shell.execute_reply.started": "2025-02-03T06:50:46.618960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world, World. Hello world, world. This is a test!\n"
     ]
    }
   ],
   "source": [
    "def remove_adjacent_duplicates(text):\n",
    "    if not text:\n",
    "        return \"\"  # Return empty string if input is empty\n",
    "\n",
    "    sentences = sent_tokenize(text)  # Split text into sentences\n",
    "    detokenizer = TreebankWordDetokenizer()\n",
    "    cleaned_sentences = []\n",
    "    prev_sentence = None\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)  # Tokenize sentence into words\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        # Remove adjacent duplicate words (Case-Sensitive)\n",
    "        filtered_words = [words[0]]\n",
    "        for i in range(1, len(words)):\n",
    "            if words[i] != words[i - 1]:  # Case-sensitive comparison\n",
    "                filtered_words.append(words[i])\n",
    "\n",
    "        cleaned_sentence = detokenizer.detokenize(filtered_words)  # Proper spacing\n",
    "\n",
    "        # Avoid adding duplicate adjacent sentences (Case-Sensitive)\n",
    "        if cleaned_sentence != prev_sentence:\n",
    "            cleaned_sentences.append(cleaned_sentence)\n",
    "            prev_sentence = cleaned_sentence  # Keep case-sensitive tracking\n",
    "\n",
    "    return \" \".join(cleaned_sentences)  # Proper spacing between sentences\n",
    "\n",
    "text = \"Hello world, World. Hello world, world. This is is a test test!  This is is a test test!\"\n",
    "print(remove_adjacent_duplicates(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c975a8-5d7c-482f-9006-9fe96467665d",
   "metadata": {},
   "source": [
    "## Assignment --> Use Case 2: Adding Custom Stop Words to `nltk` and `spacy`\n",
    "\n",
    "\n",
    "Adding custom stop words is a crucial preprocessing step in NLP. This task is important because:\n",
    "\n",
    "Customizing stop words allows for more flexible and relevant text cleaning tailored to specific use cases.\n",
    "Adding domain-specific stop words improves the performance of text analysis and machine learning models by removing irrelevant terms.\n",
    "Enhances the readability and coherence of the text by eliminating non-essential words.\n",
    "\n",
    "Objective:\n",
    "Extend the default stop words list in both `nltk` and `spacy` by adding custom stop words.\n",
    "\n",
    "Input:\n",
    "A list of custom stop words to be added to the existing stop words list in `nltk` and `spacy.\n",
    "\n",
    "Output:\n",
    "A function that takes a string and returns the text with both default and custom stop words removed.\n",
    "\n",
    "Constraints:\n",
    "The input string can be empty.\n",
    "The words are case-sensitive, meaning \"Word\" and \"word\" are considered different.\n",
    "\n",
    "Instructions:\n",
    "Add custom stop words to `nltk`'s default stop words list.\n",
    "Add custom stop words to `spacy`'s default stop words list.\n",
    "Remove stop words from a given text using the updated stop words list for both `nltk` and `spacy.\n",
    "\n",
    "**Note: Please ensure that the custom stop words you add are unique to your implementation. When testing and checking your notebooks, I will include these specific words to ensure they have been correctly added to your stop words list.**\n",
    "\n",
    "Custom Stop Words to Use:    \n",
    "\"customword1\";  \n",
    "\"customword2\";  \n",
    "\"customword3\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2be6a7-432d-40f0-8efa-eea46ca02904",
   "metadata": {},
   "source": [
    "### Add custom words here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cf76b6c7-48b5-48b3-aa0a-8f6c69eebf16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T07:43:55.058621Z",
     "iopub.status.busy": "2025-02-03T07:43:55.058115Z",
     "iopub.status.idle": "2025-02-03T07:43:55.061639Z",
     "shell.execute_reply": "2025-02-03T07:43:55.061639Z",
     "shell.execute_reply.started": "2025-02-03T07:43:55.058621Z"
    }
   },
   "outputs": [],
   "source": [
    "custom_stop_words = {\"cat\", \"dog\", \"sheep\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "443bab28-2b17-4361-9059-298f0e468ed3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T07:46:02.736931Z",
     "iopub.status.busy": "2025-02-03T07:46:02.736931Z",
     "iopub.status.idle": "2025-02-03T07:46:03.219359Z",
     "shell.execute_reply": "2025-02-03T07:46:03.219359Z",
     "shell.execute_reply.started": "2025-02-03T07:46:02.736931Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Extend NLTK stop words\n",
    "nltk_stop_words = set(stopwords.words(\"english\")).union(custom_stop_words)\n",
    "\n",
    "# Extend spaCy stop words\n",
    "for word in custom_stop_words:\n",
    "    nlp.Defaults.stop_words.add(word)\n",
    "    nlp.vocab[word].is_stop = True\n",
    "\n",
    "def remove_stop_words_nltk(text: str) -> str:\n",
    "    \"\"\"Removes stop words from the given text using nltk.\"\"\"\n",
    "    if not text.strip():  # Handle empty input\n",
    "        return \"\"\n",
    "\n",
    "    # Tokenize using NLTK\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words using NLTK's list\n",
    "    filtered_words = [word for word in words if word.lower() not in nltk_stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "def remove_stop_words_spacy(text: str) -> str:\n",
    "    \"\"\"Removes stop words from the given text using spaCy.\"\"\"\n",
    "    if not text.strip():  # Handle empty input\n",
    "        return \"\"\n",
    "\n",
    "    # Process text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41126851-c2e4-4a22-8ef2-ad0323bdfc87",
   "metadata": {},
   "source": [
    "### nltk version test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9a58b10b-2665-44b5-b650-f32f0b46e904",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T07:46:03.378668Z",
     "iopub.status.busy": "2025-02-03T07:46:03.377668Z",
     "iopub.status.idle": "2025-02-03T07:46:03.382787Z",
     "shell.execute_reply": "2025-02-03T07:46:03.382276Z",
     "shell.execute_reply.started": "2025-02-03T07:46:03.378668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'text', ',', ',', 'words', '.']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"This is a sample text with cat, dog, sheep and other words.\"\n",
    "cleaned_text = remove_stop_words_nltk(text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3921b2f-214a-4a7b-890e-7473b85a70ab",
   "metadata": {},
   "source": [
    "### spacy version test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c8f7345e-cc0e-4e21-8b32-0c4c32e293f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T07:46:04.137375Z",
     "iopub.status.busy": "2025-02-03T07:46:04.137375Z",
     "iopub.status.idle": "2025-02-03T07:46:04.150483Z",
     "shell.execute_reply": "2025-02-03T07:46:04.149978Z",
     "shell.execute_reply.started": "2025-02-03T07:46:04.137375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'text', ',', ',', 'words', '.']\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = remove_stop_words_spacy(text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b006013-8b9b-47b1-9e84-0aa4a4a0871f",
   "metadata": {},
   "source": [
    "## Assignment --> Use Case 3: `nltk` Stemming\n",
    "\n",
    "Objective:\n",
    "\n",
    "Understand and compare the stemming techniques. Determine when each stemming technique is appropriate to use based on the context and requirements.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Apply stemming using `PorterStemmer`, `LancasterStemmer`, and `SnowballStemmer`.\n",
    "\n",
    "Compare the results and analyze the differences.\n",
    "\n",
    "Write code to demonstrate the stemming process for each stemmer.\n",
    "Provide example text and show the output of each stemming process.\n",
    "\n",
    "Analysis:\n",
    "\n",
    "Discuss the differences between the stemmers.\n",
    "Explain when one stemmer might be more appropriate than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e6621995-f6c4-411b-84bc-afbf8902a145",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T07:53:16.408478Z",
     "iopub.status.busy": "2025-02-03T07:53:16.407494Z",
     "iopub.status.idle": "2025-02-03T07:53:16.413044Z",
     "shell.execute_reply": "2025-02-03T07:53:16.413044Z",
     "shell.execute_reply.started": "2025-02-03T07:53:16.408478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: ['running', 'jumps', 'happily', 'flies', 'better', 'swimming', 'flying']\n",
      "Porter Stemmer: ['run', 'jump', 'happili', 'fli', 'better', 'swim', 'fli']\n",
      "Lancaster Stemmer: ['run', 'jump', 'happy', 'fli', 'bet', 'swim', 'fly']\n",
      "Snowball Stemmer: ['run', 'jump', 'happili', 'fli', 'better', 'swim', 'fli']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "# Example text\n",
    "text = [\"running\", \"jumps\", \"happily\", \"flies\", \"better\", \"swimming\", \"flying\"]\n",
    "\n",
    "# Initialize the stemmers\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Apply stemming using each stemmer\n",
    "porter_stemmed = [porter_stemmer.stem(word) for word in text]\n",
    "lancaster_stemmed = [lancaster_stemmer.stem(word) for word in text]\n",
    "snowball_stemmed = [snowball_stemmer.stem(word) for word in text]\n",
    "\n",
    "# Print the results\n",
    "print(\"Original text:\", text)\n",
    "print(\"Porter Stemmer:\", porter_stemmed)\n",
    "print(\"Lancaster Stemmer:\", lancaster_stemmed)\n",
    "print(\"Snowball Stemmer:\", snowball_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe2a3d-adad-4583-ac74-024177e311bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
